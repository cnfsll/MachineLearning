比较分别使用SGD和ADAM优化方法的效果。其中使用SGD的代码和结果见/Tanh-Relu中使用relu激励函数的情况。 
实验结果表示ADAM的效果更好，收敛速度更快，测试准确率更高（99.06% VS 98.85%）。
mnist_adam.py：使用ADAM优化方法的代码； sgd_adam.pny：训练准确率变化比较图；
